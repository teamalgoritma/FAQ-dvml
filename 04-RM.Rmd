# Regression Model

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)

# scientific notation
options(scipen = 9999)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# libraries
library(tidyverse)
library(fastDummies)
```

## Linear Regression

### **Bagaimana model regresi linier dengan fungsi `lm()` bekerja jika terdapat prediktor bertipe kategorik?**

Fungsi `lm()` pada R akan secara otomatis mengubah prediktor bertipe kategorik menjadi variabel dummy. Variabel dummy adalah hasil transformasi prediktor kategorik berupa nilai 0 atau 1 untuk menggambarkan ada/tidaknya suatu kategori (metode untuk mengkuantitatifkan prediktor bertipe kategorik). Jika terdapat $k$ kategori pada suatu prediktor bertipe kategori, maka akan dihasilkan sejumlah $k-1$ variabel dummy. Misal, jika terdapat 1 kolom berisi prediktor bertipe kategorik yang menggambarkan tingkat salary dari setiap pelanggan sebagai berikut
```{r echo=FALSE}
salary_level <- data.frame(salary_level = factor(c("High", "Low", "Medium", "Low", "High")))
salary_level
```

Maka, akan dihasilkan 2 variabel dummy seperti berikut
```{r echo=FALSE}
dummy_cols(salary_level, remove_first_dummy = T) %>% 
  select(-salary_level)
```

### **Jika pada hasil summary model regresi linier terdapat salah satu level/kategori dari prediktor bertipe kategorik yang tidak signifikan mempengaruhi target variabel (p-value < alpha), apakah prediktor tersebut signifikan mempengaruhi target variabel atau tidak?**

Keadaan tersebut biasanya disebabkan oleh jumlah sampel untuk level/kategori tersebut tidak cukup banyak dibandingkan dengan jumlah sampel level/kategori lainnya. Kesimpulan signifikan atau tidaknya kembali pada masing-masing user dengan mempertimbangkan sudut pandang bisnis dan permasalahan yang dianalisis. Namun, salah satu solusi yang dapat dicoba adalah menggabungkan sampel dengan level/kategori tersebut dengan level/kategori lainnya yang memiliki karakteristik yang serupa.

### **Fungsi `lm()` akan secara otomatis mengubah prediktor bertipe kategorik menjadi variabel dummy dimana level/kategori paling awal akan dijadikan sebagai basis (dihilangkan). Apakah hasil analisis model regresi linier akan berubah, jika dilakukan pengurutan ulang (reorder) level/kategori dari suatu prediktor bertipe kategorik?**

Hasil analisis model regresi linier yang diperoleh tidak akan berubah, melakukan pengurutan ulang level/kategori dari suatu prediktor numerik hanya akan mengubah basis yang digunakan. 

### **Jika pada hasil summary model regresi linier terdapat prediktor yang tidak signifikan, apakah prediktor tersebut lebih baik tidak diikutsertakan dalam model regresi linier atau sebaliknya?**

Hal tersebut bergantung pada sudut pandang bisnis dan permasalahan yang dianalisis, jika berdasarkan sudut pandang bisnis prediktor tersebut harus diketahui bagaimana dan seberapa besar pengaruhnya terhadapap target variabel. Maka, tetap dapat diikutsertakan. Namun, jika berdasarkan sudut pandang bisnis tidak terlalu mempengaruhi target variabel, maka boleh tidak diikutsertakan dengan tujuan supaya model regresi linier yang dihasilkan menjadi lebih sederhana. Solusi lain yang dapat dipertimbangkan untuk dilakukan adalah menambah jumlah sampel atau membuang observasi yang merupakan outlier dan berpengaruh negatif terhadap hasil analisisi regresi linier. Hal ini dapat dipertimbangkan untuk dilakukan karena suatu prediktor tidak signifikan mempengaruhi target tidak hanya karena antara prediktor dengan target tidak saling mempengaruhi dan dipengaruhi, melainkan juga karena beberapa faktor seperti variansi data yang rendah atau karena terdapat observasi outlier yang justru berpengaruh negatif terhaddap hasil analisis regresi linier.

### **Apa saja cara yang dapat dilakukan untuk memperbaiki model regresi linier (tuning)?**

Berikut beberapa cara yang dapat dilakukan untuk memperbaiki model regresi linier:

1. Deteksi observasi outlier
   - Pengaruh negatif terhadap model regresi linier sebaiknya observasi tersebut tidak diikutsertakan 
   - Pengaruh positif terhadap model regresi linier sebaiknya observasi tersebut tetap dapat diikutsertakan
2. Menambah prediktor berdasarkan informasi dari prediktor yang sudah ada (feature engineering)
3. Melakukan transformasi data
   - Mengubah prediktor bertipe numerik menjadi kategorik
   - Melakukan transformasi pada prediktor bertipe numerik dengan operasi matematika seperti log, ln, sqrt, kuadrat, dll

### **Apa kegunaan nilai p-value pada hasil summary model regresi linier, padahal biasanya dilakukan pemeriksaan nilai korelasi atau tes/uji korelasi sebelum melakukan pemodelan regresi linier?**

Memeriksa nilai korelasi atau melakukan tes/uji korelasi bertujuan untuk melihat keterkaitan antara prediktor dengan target variabel, dimana secara logika (hipotesis) variabel yang saling berkaitan ada kemungkinan untuk saling mempengaruhi dan dipengaruhi. Sementara, nilai p-value pada hasil summary model regresi linier digunakan untuk mengkonfirmasi apakah antara prediktor dan target saling mempengaruhi dan dipengaruhi. Sebab, korelasi hanya menyatakan keterkaitan antar variabel saja, bukan menjelaskan hubungan sebab dan akibat.

#### **Apakah regresi linier dapat dilakukan jika target variabel bertipe numerik diskrit?**

Bisa saja, namun hasil prediksinya kurang tepat karena menghasilkan nilai numerik kontinu yang kemungkinan menghasilkan error yang lebih besar. Model regresi yang lebih cocok digunakan untuk kondisi tersebut adalah model regresi poisson, untuk penjelasan lengkapnya dapat membaca link referensi berikut [Regresi Poisson](https://algotech.netlify.com/blog/poisson-regression-and-neg-ative-binomial-regression/)

### **Apakah cukup dengan mengecek nilai korelasi atau harus dilakukan tes/uji korelasi untuk mengecek keterkaitan antara prediktor dengan target variabel?**

Nilai korelasi di bawah -0.5 atau di atas 0.5 sudah dianggap bahwa antara prediktor dan target variabel memiliki keterkaitan/hubungan yang cukup kuat. Namun, sebaliknya nilai korelasi di antara -0.5 sampai 0.5 mengindikasikan bahwa antara prediktor dengan target saling berkaitan/berhubungan, tetapi hubungannya lemah. Sehingga, perlu dilakukan tes/uji korelasi untuk mengonfirmasi apakah hubungan antara prediktor dengan target variabel signifikan atau tidak secara statistik (objektif).

### **Hasil summary model regresi linier dari fungsi `lm()` memuat beberapa nilai seperti std.error, t-value, dan p-value. Apa kegunaan dari ketiga nilai tersebut?**

Std.error dan t-value pada hasil summary model regresi linier digunakan untuk menghitung nilai p-value. Dimana nilai p-value digunakan untuk melakukan tes/uji hipotesis untuk menguji apakah prediktor signifikan mempengaruhi target variabel atau tidak. 
Hipotesis:

- H0: Prediktor tidak mempengaruhi target variabel
- H1: Prediktor mempengaruhi target variabel
	
Nilai p-value tersebut harus dibandingkan dengan alpha yang digunakan (p-value < alpha: prediktor signifikan mempengaruhi target variabel) untuk menarik kesimpulan apakah prediktor signifikan mempengaruhi target atau tidak. Namun, hasil summary model regresi linier di R sudah dilengkapi oleh simbol bintang yang mempermudah user untuk menarik kesimpulan. Jika minimal terdapat satu simbol bintang, maka prediktor signifikan mempengaruhi target variabel dengan alpha 5% (p-value < 0.05).

### **Bagaimana cara menampilkan nilai p-value pada output linear model (regresi linier)?**

Output dari fungsi `lm()` di R dapat dilihat melalui fungsi `summary()`. Output tersebut berupa list yang dapat diakses sesuai dengan aturan indexing dan subsetting pada list. 
```{r}
lm_mtcars <- lm(mpg ~ cyl, mtcars)
summary(lm_mtcars)
```

Untuk mengambil nilai pada index `coefficient`, dapat menggunakan fungsi `summary(model)$coefficient` yang akan menghasilkan output berupa matrix
```{r}
summary(lm_mtcars)$coefficient
```

Kemudian untuk mengambil p-value dari matrix di atas dapat menggunakan aturan indexing dan subsetting pada matrix, dimana nilai yang ingin diambil berada pada kolom ke-4
```{r}
summary(lm_mtcars)$coefficient[, 4]
```

## Evaluation

### **Apakah nilai AIC bisa negatif?**

AIC dapat bernilai negatif atupun positif bergatung pada nilai maksimum likelihood yang diperoleh. Berikut formula untuk menghitung nilai AIC:

$$AIC = 2k - 2ln_{L}$$

Dimana $k$ merupakan jumlah parameter yang diprediksi (prediktor dan intersep) dan $L% merupakan nilai maksimum likelihood yang diperoleh.

Namun, pemilihan model regresi linier pada metode stepwise regression tidak memperhatikan tanda negatif atau positif dari nilai AIC melainkan nilainya saja (absolut AIC). Sehingga, tanda negatif atau positif pada nilai AIC tidak berpengaruh dalam proses pemilihan model regresi linier pada metode stepwise regression. Model yang dipilih adalah model yang memiliki nilai abolut AIC terkecil yang mengindikasikan bahwa semakin sedikit informasi yang hilang pada model tersebut. [Negative values for AIC](https://stats.stackexchange.com/questions/84076/negative-values-for-aic-in-general-mixed-model)

### **Apa perbedaan R-squared dan Adjusted R-squared?**

- R-Squared: Seberapa baik model menjelaskan data, dengan mengukur seberapa besar informasi (variansi) dari target dapat dijelaskan oleh prediktor. Sehingga, jelas ketika **prediktor bertmabah**, informasi (variansi) yang dirangkum semakin banyak atau dengan kata lain jelas **nilai R-Squared akan meningkat**.
- Adj. R- Squared: tidak demikian pada adj. r-squared, karena disesuaikan dengan jumlah prediktor yang digunakan. Adj. r-squared akan meningkat hanya jika prediktor baru yang ditambahkan mengarah pada hasil prediksi yang lebih baik (prediktor signifikan mempengaruhi target)

Formula Adj. R-Squared:

$$R^2_{adj} = 1-(1-R^2)\frac{n-1}{n-p-1}$$

Dimana, $n$ adalah jumlah sampel dan $p$ adalah jumlah prediktor.

### **Adakah batasan nilai R-squared yang dianggap baik?**

Baik tidaknya nilai R-squared adalah relatif bergantung pada sudut pandang bisnis dan permasalahan yang dianalisis. Namun, umumnya nilai R-squared di atas atau sama dengan 70% sudah cukup baik dalam menjelaskan variansi dari target variabel.

### **Apakah hanya nilai AIC yang dapat digunakan untuk memilih model regresi linier pada stepwise regression?**

Secara default fungsi `stepwise()` di R hanya menngunakan nilai AIC untuk memilih model regresi linier. Jika ingin membandingkan metrics evaluasi lain dapat menggunakan fungsi `compare_performance()` dari package `performance` untuk menampilkan beberapa nilai metrics evaluasi seperti R-squared, BIC, dll untuk setiap kombinasi model yang dibuat. Berikut beberapa alasan mengapa nilai AIC lebih diutmakan untuk memilih model regresi linier dibandingkan metrics lainnya [Alasan Penggunaan nilai AIC](https://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other)

### **Stepwise regression memiliki 3 algoritma, yaitu backward, forward, dan both. Apakah ketiga algoritma tersebut memiliki fungsi yang sama?**

Stepwise regression baik algoritma backward, forward, ataupun both memiliki fungsi yang sama, yaitu untuk melakukan pemilihan prediktor (variable selection) yang akan diikutsertakan dalam model regresi linier. Stepwise regession baik dengan algoritma backward, forward, ataupun both akan menghasilkan performa model yang hampir sama, namun ada baiknya sebagai user melakukan trial and error dari ketiga algoritma tersebut. Kemudian, memilih model regresi linier yang memiliki R-squared adjusted tertinggi dan nilai AIC paling kecil. 

### **Adakah batasan nilai AIC yang dianggap kecil?**

Nilai AIC memiliki interval dari -inf sampai +inf, sehingga tidak ada ketentuan yang menyatakan berapa nilai AIC yang dianggap kecil. Namun, untuk mengetahui apakah kombinasi model regresi linier yang dihasilkan sudah baik atau belum, user dapat melihat penurunan/peningkatan nilai AIC. Jika ketika melakukan stepwise regression baik dengan metode forward, backward, ataupun both nilai AIC sudah tidak mengalami penurunan, justru mengalami kenaikan ketika ditambah/dikurangkan prediktornya, maka iterasi sudah berhenti pada kombinasi model sebelum terjadi peningkatan nilai AIC.

## Assumption

### **Bagaimana menguji asumsi pada linear regression menggunakan uji statistik?**

**1. Normality of Residuals **

Model linear regression diharapkan menghasilkan **error yang berdistribusi normal**. Dengan begitu, error lebih banyak berkumpul di sekitar angka nol. Asumsi normalitas dapat diuji menggunakan `shapiro.test()`, dengan uji hipotesis sebagai berikut:

* H0: error berdistribusi normal
* H1: error tidak berdistribusi normal

```{r,  eval=FALSE}
shapiro.test(model$residuals)
```

Kondisi yang diharapkan adalah error berdistribusi normal (H0 tidak ditolak), yaitu p-value > 0.05 (tingkat error). 

**2. Homoscedasticity of Residuals**

Diharapkan error yang dihasilkan oleh model menyebar secara acak atau dengan variasi konstan. Apabila divisualisasikan maka error tidak berpola. Kondisi ini disebut juga sebagai homoscedasticity. Homoscedasticity dapat dicek dengan uji statistik `bptest()` dari package `lmtest`

Uji hipotesis Breusch-Pagan:

* H0: error menyebar konstan atau homoscedasticity
* H1: error menyebar TIDAK konstan atau heteroscedasticity

```{r, eval=FALSE}
library(lmtest)
bptest(model)
```

Kondisi yang diharapkan adalah error menyebar konstan (H0 tidak ditolak), yaitu p-value > 0.05 (tingkat error). 

**3. No Multicollinearity**

Multicollinearity adalah kondisi adanya korelasi antar prediktor yang kuat. Hal ini tidak diinginkan karena menandakan prediktor redundan pada model, yang seharusnya dapat dipilih salah satu saja dari variabel yang hubungannya amat kuat tersebut. Harapannya tidak terjadi multicollinearity

Uji VIF (Variance Inflation Factor) dengan fungsi `vif()` dari package `car`:

```{r, eval=FALSE}
library(car)
vif(model_backward)
```

* nilai VIF > 10: terjadi multicollinearity pada model
* nilai VIF < 10: tidak terjadi multicollinearity pada model

### **Mengapa pada asumsi normality yang harus berdistribusi normal adalah error/residual?**

Nilai error/residual yang diharapkan untuk setiap analisis termasuk regrsi linier adalah 0, untuk memastikan bahwa hasil prediksi untuk setiap observasi mendekati atau sama dengan 0, maka diharapkan error/residual berdistribusi normal dimana nilai rata-rata error/residual sama dengan 0. Error/residual pada regresi linier seringkali tidak berdistribusi normal disebabkan oleh beberap faktor, yaitu:

-  Model yang digunakan tidak cocok, misal hubungan antara prediktor dengan target variabel tidak linier melainkan kudratik/eksponensial/dll
-  Terdapat observasi outlier

### **Mengapa perlu dilakukan pengecekkan asumsi model regresi linier?**

Supaya interpretasi dan hasil prediksi dari model regresi linier bersifat `BLUE` (Best, Linear, Unbiased Estimator). Secara sederhana, hasil analisis regresi linier dapat berlaku secara objektif dan konsisten.

### **Jika sudah mencoba berbagai solusi untuk memperbaiki model regresi linier supaya asumsi terpenuhi, tetapi masih terdapat asumsi yang tidak terpenuhi apa yang harus dilakukan?**

Hal tersebut berarti kondisi data historis yang tersedia tidak cocok dianalisis menggunakan model regresi linear. Anda dapat mencoba menganalisis data tersebut menggunakan metode/model lainnya seperti tree based method (regression tree) yang juga dapat dinterpretasikan dan free assumption.

### **Bagaiamna cara melakukan tes/uji normality terhadap error/residual jika jumlah sampel yang digunakan lebih dari 5000?**

Anda dapat melakukan tes/uji normality terhadap error/residual menggunakan uji Kolmogorov Smirnov. Berikut contoh cara melakukan uji Kolmogorov Smirnov di R:
```{r, eval=FALSE}
ks.test(model$residuals, "pnorm", mean = mean(model$residuals), sd = sd(model$residuals))
```

### **Bagaimana cara melakukan tes/uji korelasi antara prediktor dengan target variabel secara langsung untuk semua prediktor di R?**

Fungsi yang digunakan untuk melakukan tes/uji korelasi di R adalah `cor.test()` sebagai berikut:
```{r}
copiers <- read.csv("data/03-RM/copiers.csv") %>% select(-Row.ID)
cor.test(copiers$Sales, copiers$Profit)
```

Dengan hipotesis

- H0: Prediktor dan target tidak saling berkorelasi
- H1: Prediktor dan target saling berkorelasi

Anda dapat membuat suatu fungsi yang mengaplikasikan fungsi `cor.test()` di atas untuk setiap prediktor sebagai berikut:
```{r}
cor.test.all <- function(data,target) {
  names <- names(data %>% select_if(is.numeric))
  df <- NULL
  for (i in 1:length(names)) {
    y <- target
    x <- names[[i]] 
    p_value <- cor.test(data[,y], data[,x])[3]
    temp <- data.frame(x = x,
                       y = y,
                       p_value = as.numeric(p_value))
    df  <- rbind(df,temp)
  }
  return(df)
}

cor.test.all(data = copiers, target ="Profit")
```

### **Apa akibat jika asumsi no-multicolinearity tidak terpenuhi?**

Model regresi linier yang dihasilkan menjadi tidak efisien karena terdapat informasi yang redundan (sama). Ada baiknya model regresi linier yang dipilih adalah model regresi linier yang paling efisien dan sederhana dengan performa yang cukup baik (error yang dihasilkan relatif kecil)

## Mathematics Formula

Untuk mengestimasi nilai koefisien (beta), pertama-tama coba ingat kembali beberapa konsep pada workshop "Practical Statistics". Variance merupakan nilai yang menggambarkan seberapa bervariasi/beragamnya suatu variabel bertipe numerik/angka. Semakin besar nilai variance maka semakin beragam nilai dalam satu variabel (heterogen), sedangkan semakin kecil nilai variance maka semakin sama/mirip setiap observasi pada satu variabel (homogen). Data yang observasinya bernilai sama, maka variance sama dengan 0.

Sementara covariance merupakan nilai yang menggambarkan hubungan (positif/negatif/tidak ada hubungan) antara dua variabel numerik. Namun covariance tidak dapat menggambarkan seberapa erat/kuat hubungan tersebut karena nilai covariance tidak memilki batasan yang mutlak (- inf, + inf).

Dalam notasi matematika, anggap kita memiliki data yang terdiri dari 2 variabel, yaitu, $({X_i}, {Y_i})$, maka secara empiris nlai covariance diperoleh dari:  

$$Cov(X,Y) = \frac{1}{n-1}\sum\limits_{i=1}^{n}({X_i}-\bar{X})({Y_i}-\bar{Y})$$    

bisa juga diperoleh dari,

$$Cov(X,Y) = \frac{1}{n-1}(\sum\limits_{i=1}^{n}{X_i}{Y_i} - n\bar{X}\bar{Y})$$  

Jika formula dari covariance cukup rumit, coba ingat kembali formula dari variance:

$$S^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n}(X_i - \bar{X})^2$$  

pahami bahwa perbedaan variance dan covariance adalah variance hanya mengacu pada 1 variabel, sedangkan covariance mengacu pada 2 variabel. Maka, formula dari covariance:  

$$Cov(X,Y) = \frac{1}{n-1}\sum\limits_{i=1}^{n}({X_i}-\bar{X})({Y_i}-\bar{Y})$$  

Seperti yang telah dijelaskan di atas bahwa covariance menjelaskan jenis hubungan antara 2 variabel numerik. Namun, kita tidak dapat menilai seberapa erat/kuat hubungan antara keduanya karena interval nilai covariance yang tidak memiliki batasan. Oleh karena itu, kita bisa melakukan standarization terlebih dahulu terhadap 2 variabel numerik tersebut yang mengacu pada definisi correlation:

$$Cor(X, Y) = \frac{Cov(X,Y)}{{S_x}{S_y}}$$

beberapa fakta mengenai correlation:

- Cor(X,Y) = Cor(Y,X)  
- -1 <= Cor(X,Y) <= 1  
- Nilai correlation mendekati 1 artinya kedua variabel berhubungan erat dan hubungannya linier positif  
- Nilai correlation mendekati -1 artinya kedua variabel berhubungan erat dan hubungannya linier negatif
- Nilai correlation mendekati 0 artinya kedua variabel tidak saling berhubungan secara linier  

Untuk menggambarkan persebaran observasi antara x dan y, dapat dilakukan dengan menarik suatu garis lurus yang menggambarkan keseluruhan persebaran data. Dimana, untuk menarik suatu garis lurus diperlukan titik awal ($b0$) dan kemiringan garis ($b1$).

Lalu bagaimana cara mengestimasi $b0$ dan $b1$ yang optimal (dimana garis linier dapat menggambarkan keseluruhan persebaran data). Kita bisa menggunakan konsep kuadrat terkecil, untuk menemukan kombinasi $b0$ dan $b1$ yang meminimumkan jarak kuadrat antara titik pengamatan dengan garis linier:

$$\sum\limits_{i=1}^{n}\{{Y_i} - (\beta_0 + {\beta_1}{X_i} )\}^2$$

- Estimasi slope: 

$$\hat{\beta}_1 = Cor(Y,X)\frac{Sd(Y)}{Sd(X)}$$  

- Estimasi intercept: 

$$\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$$

**R-square** secara definisi adalah persentase total keragaman suatu target variabel yang dapat dijelaskan oleh prediktor variabel (model), dengan formula:  

$$R^2 = 1 - \frac{\sum\limits_{i=1}^n (\hat{Y_i} - \bar{Y})^2}{\sum\limits_{i=1}^n(Y_i - \bar{Y})^2}$$  

Beberapa fakta tentang R-square ($R^2$):


- $R^2$ adalah persentase total keragaman suatu target variabel yang dapat dijelaskan oleh model regresi  
- $0 \leq R^2 \leq 1$  

Namun, penggunaan R-square seringkali keliru karena adanya batasan dalam penggunaan metrik ini. R-square cenderung meningkat setiap penambahan variabel prediktor, walaupun variabel prediktor tersebut tidak mempengaruhi variabel target secara signifikan. Akibatnya, model dengan variabel prediktor yang lebih banyak mungkin tampak lebih baik hanya karena memiliki lebih banyak variabel prediktor saja.

Berbeda dengan **Adjusted R-square** yang tidak mengalami peningkatan setiap penambahan variabel prediktor karena adjusted R-square meningkat hanya ketika variabel prediktor baru benar-benar mengarah ke prediksi yang lebih baik (signifikan mempengaruhi variabel target).

Formula adjusted R-squared:

$$R^2_{adj} = 1-(1-R^2)\frac{n-1}{n-p-1}$$

Dimana $n$ adalah jumlah pengamatan dan $p$ adalah jumlah prediktor. Perhatikan bahwa ketika $p$ meningkat, $\frac{n-1}{n-p-1}$ akan mengecil dan mendorong nilai adjusted R-square secara keseluruhan menjadi kecil.

Salah satu alat statistik yang dapat digunakan untuk mengecek ada/tidak multicolinearity adalah **Variance Inflation Factor** (VIF). VIF mengukur peningkatan estimasi koefisien beta, jika antar prediktor saling berkorelasi. Secara matematis, nilai VIF diperoleh dengan meregresikan setiap prediktor dengan prediktor lain. Contoh: diketahui terdapat $X1, X2, ..., Xn$, nilai VIF untuk $X1$ diperoleh dari hasil regresi $X1$ dengan $X2, ..., Xn$, dst. Hasil regresi tersebut kemudian diterapkan pada formula berikut:

$$VIF = \frac{1}{1-R^2(x)}$$

Secara umum jika nilai VIF yang diperoleh lebih besar atau sama dengan 10, mengindikasikan terjadi multicolinearity (antar prediktor saling berkorelasi kuat).


