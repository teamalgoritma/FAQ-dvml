[
["FAQ.html", "FAQ Forewords Disclaimer Chapter 1 Regression Model Chapter 2 Classification 1 2.1 Imbalance Target Variable Chapter 3 Classification 2 Chapter 4 Unsupervised Learning Chapter 5 Time Series and Forecasting Chapter 6 Neural Network and Deep Learning", " FAQ Team Algoritma January 22, 2020 Forewords Algoritma providing workshop to help working professionals and students to achieve basic and advanced skill in various data science sub-fields, including: data visualization, machine learning, data modeling, statistical inference etc. Disclaimer This coursebook is produced by Team Algoritma for its Data Science Academy workshops. No part of this coursebook may be reproduced in any form without permission from the author, and is inteded for restricted audiences that participate on the workshop. Chapter 1 Regression Model Bagaimana penanganan data kategorik pada model linear regression? Dengan menggunakan function lm pada R otomatis akan mengubah tipe data kategorik menjadi dummy variabel. Dummy variable berfungsi untuk mengkuantitatifkan variabel yang bersifat kualitatif (kategorik). Dummy variabel hanya mempunyai dua nilai yaitu 1 dan 0. Dummy memiliki nilai 1 untuk salah satu kategori dan nol untuk kategori yang lain. Jika terdapat sebanyak k kategori untuk suatu prediktor maka akan ditransformasi menjadi k-1 dummy. Mengapa untuk asumsi normality yang harus berdistribusi normal adalah error/residual ? Jika residual berdistribusi normal, itu artinya residual cenderung berkumpul di titik sekitar 0, dapat dikatakan hasil prediksi tidak terlalu melenceng jauh dari data actual. Error yang tidak berdistribusi normal disebabkan oleh: distribusi target variabel memang tidak normal Model yang digunakan tidak cocok, misal hubungan antara prediktor dengan target tidak linier melainkan kudratik/eksponensial/dll walaupun target variabel memiliki distribusi normal. Selain itu, error harus berdistribusi normal terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval). Terdapat banyak data outlier Mengapa untuk asumsi normality yang harus berdistribusi normal adalah error/residual ? Karena ingin melihat kecocokan model. Error yang tidak berdistribusi normal disebabkan oleh: distribusi target variabel memang tidak normal Model yang digunakan tidak cocok, misal hubungan antara prediktor dengan target tidak linier melainkan kudratik/eksponensial/dll walaupun target variabel memiliki distribusi normal. Selain itu, error harus berdistribusi normal terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval). Untuk prediktor kategorik, bagaimana jika terdapat kategori yang tidak signifikan (p value &gt; alpha)? apakah prediktor tersebut masih dianggap signifikan mempengaruhi target? Untuk variabel kategorik ketika salah satu variabel signifikan, kita anggap levels lainnya juga signifikan. Pada fungsi lm sudah otomatis melakukan transformasi data kategorik dengan level pertama yang dijadikan basis. Bagaimana jika dilakukan reorder level (mengubah urutan level), apakah akan mengubah hasil pemodelan? Hasil pemodelan tidak akan berubah, mengubah urutan level hanya akan mengubah basis yang digunakan. Bagaimana jika terdapat prediktor yang tidak signifikan, tetapi secara bisnis seharusnya prediktor tersebut berpengaruh terhadap target? Ketika variabel yang kita gunakan tidak signifikan secara statistik, namun secara bisnis berpengaruh terhadap target yang dimiliki, kita akan tetap mempertahankan variabel tersebut, banyak faktor yang menyebabkan variabel tersebut tidak signikan, bisa jadi karena data yang dimiliki tidak cukup banyak, banyak data oulier, atau ragam data yang hanya sedikit. Mengapa perlu dilakukan pengecekkan asumsi pada metode regresi linier? Pengecekkan asumsi dilakukan terkait dengan pengujian-pengujian parameter (beta/koefisien regresi) secara statistik (F-test, t-test, dan confidence interval). Ketika semua asumsi terpenuhi model dapat dikatakan BLUE (Best Linear Unbiased Estimator). Jika sudah dilakukan berbagai alternatif untuk pemenuhan asumsi, namun masih terdapat asumsi yang tidak terpenuhi apa yang harus dilakukan? Ketika sudah dilakukan berbagai alternatif untuk memenuhi asumsi namun asumsi masih tidak terpenuhi, itu artinya data yang kita miliki tidak cocok menggunakan regresi linear, dapat dicoba dengan model lain. Bagaimana jika diperoleh nilai AIC negatif? AIC dapat bernilai negatif atupun positif yang disebabkan oleh nilai dari fungsi maksimum likelihood berikut: AIC = 2k ‚àí 2ln(L), dimana k merupakan jumlah parameter (jumlah prediktor dan intersep) dan L merupakan nilai dari fungsi maksimum likelihood. Namun, pada pemilihan model nilai AIC yang dilihat adalah nilai AIC yang sudah diabsolutkan. Sehingga tanda negatif/positif pada hasil AIC tidak berpengaruh dalam proses pemilihan model. Model yang dipilih adalah model yang memiliki nilai abolut AIC terkecil, hal ini mengindikasikan bahwa semakin sedikit model tersebut kehilangan informasi yang dibawa. Negative values for AIC Perbedaan dari R-squared dan Adjusted R-squared? R-squared memperhitungkan variasi dari semua variabel independen terhadap variabel dependen. Sehingga setiap penambahan variabel independen akan meningkatkan nilai R-squared. Sedangkan pada adjusted r-squared akan memperhitungan variasi dari variabel independen yang signifikan terhadap variabel dependen. Oleh karena itu, pada multiple linear regression disarankan untuk melihat nilai Adjusted R-squared. Apa itu outlier? Data observasi yang terlihat sangat berbeda jauh dari observasi-observasi lainnya dan muncul dalam bentuk nilai ekstrim baik untuk sebuah variabel tunggal atau kombinasi. Cara yang dapat dilakukan untuk tuning model regresi? Banyak cara yang dapat dilakukan untuk tuning model regresi, salah satunya adalah dengan deteksi outlier pada data observasi. Deteksi outlier dari data yang dimiliki, apakah dengan atau tanpa data outlier tersebut akan mengganggu perfomance model yang dimiliki. Untuk apa ada p-value di output regresi jika sebelumnya kita sudah melakukan uji korelasi? Uji korelasi pada preprocessing data dilakukan untuk melihat secara umum apakah variabel prediktor dan target terdapat hubungan kuat atau tidak. Sedangkan uji pvalue pada output regresi pada setiap variabel prediktor menyatakan apakah setiap variabel prediktor benar-benar mempengaruhi target secara statistik atau tidak. Uji statistik apa yang dapat digunakan untuk uji normalitas dengan lebih dari 5000 observasi? Uji normalitas dengan observasi yang lebih dari 5000 dapat menggunakan uji Kolmogorov Smirnov dengan code sebagai berikut: ks.test(model$residuals, &quot;pnorm&quot;, mean = mean(model$residuals), sd = sd(model$residuals)) Bagaimana jika target variabel yang dimiliki berupa bilangan diskrit, apakah bisa dilakukan analisis regresi? Untuk analisis regresi dengan target variabel berupa bilangan diskrit dapat menggunakan regressi poisson. Untuk detail lengkapnya dapat dilihat di link berikut: link Chapter 2 Classification 1 # libraries library(tidyverse) library(rsample) library(caret) library(recipes) Pada klasifikasi penentuan kelas didasarkan pada peluang, bagaimana jika peluang yang diperoleh sama besar, misal pada kasus klasifikasi biner diperoleh peluang masuk ke kelas 1 adalah 0.5 dan peluang masuk ke kelas 0 adalah 0.5? Hal tersebut bergantung pada user yang menentukan threshold/batasan probability untuk masuk ke kelas 1 atau masuk ke kelas 0. Namun, pada umumnya jika diperoleh probability &gt;= 0.5 maka observasi tersebut akan masuk ke kelas 1. Untuk prediktor kategorik, bagaimana jika terdapat kategori yang tidak signifikan (p value &gt; alpha)? apakah prediktor tersebut masih dianggap signifikan mempengaruhi target? Untuk level yang menjadi basis akan dianggap signifikan, untuk level lainnya yang tidak signifikan artinya memang level tersebut tidak memberikan pengaruh terhadap target variabel. Solusi yang dapat dilakukan adalah bining (level tersebut dijadikan satu level yg mirip dan signifikan) atau menambahkan jumlah observasi pada level yang tidak signifikan tersebut. Pada fungsi lm sudah otomatis melakukan transformasi data kategorik dengan level pertama yang dijadikan basis. Bagaimana jika dilakukan reorder level (mengubah urutan level), apakah akan mengubah hasil pemodelan? Nilai pvalue pada setiap level tidak akan berubah ketika kita melakukan reorder level. Interpretasi untuk variable kategorik bergantung pada level yang dijadikan basis Pengertian dari null deviance dan residual deviance pada output summary? Null deviance menunjukkan seberapa baik target variabel diprediksi oleh model berdasarkan nilai intercept. Sedangkan residual deviance menunjukkan seberapa baik target variabel diprediksi oleh model berdasarkan intercept dan semua variabel independen yang digunakan. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi link Apa itu Fisher Scoring pada output summary? Fisher scoring adalah turunan dari metode Newton untuk mengatasi Maximum Likelihood. Fisher scoring memberikan informasi berapa banyak iterasi yang dilakukan pada model sehingga diperoleh nilai parameter. Apa itu Maximum Likelihood Estimator (MLE)? Parameter pada model logistik regression diperoleh dari pendekatan MLE. MLE merupakan pendekatan statistik untuk memperkirakan paramater pada model. Pada kasus klasifikasi, kenapa matrix accuracy tidak cukup menjelaskan seberapa baik model yang diperoleh? Untuk mengetahui seberapa baik perfomance model klasifikasi, tidak cukup dengan melihat nilai accuracy nya saja, karena accuracy menganggap sama penting untuk nilai false positive dan false negative. Kita membutuhkan perhitungan lain seperti precision dan recall, contohnya untuk memprediksi pasien mengidap kanker jinak atau ganas. Tentunya akan lebih berbahaya jika kemampuan model yang kita miliki cenderung lebih baik memprediksi kanker ganas namun terprediksi menjadi jinak. Pada kasus ini karena kelas positif yang dimiliki adalah kanker ganas, maka kita akan mementingkan nilai recall lebih besar dibandingkan nilai pengukuran lainnya. Permasalahan apa yang paling sering ditemui pada kasus klasifikasi? Permasalahan besar kasus klasifikasi adalah dataset yang tidak seimbang. Contohnya pada case churn telekomunikasi, employee attrition, prediksi kankes, fraud detection, dan sebagainya. Dalam hal tersebut, biasanya jumlah kelas positif jauh lebih sedikit dibandingkan kelas negatif. Misalkan pada kasus fraud detection, dari 1000 transaksi yang dimiliki, 10 diantaranya fraud, sedangkan sisanya tidak fraud. Kemudian perfomance model yang diperoleh sekitar 85%, mungkin terdengar sangat baik, namun pada kenyatannya tidak. Kemungkinan besar model tersebut hanya mampu memprediksi salah satu kelas kelas mayoritas yaitu yang tidak fraud, sedangkan kelas positif yang kita miliki sangat sedikit data yang dimiliki. 2.1 Imbalance Target Variable Pemodelan klasifikasi mulai banyak digunakan pada berbagai bidang industri, seperti perbankan untuk mendeteksi transaksi yang memiliki kecenderungan kecurangan atau memprediksi potensi kegagalan nasabah dalam membayar kredit/hutang, tranportasi (penerbangan) untuk memprediksi kemungkinan suatu penerbangan mengalami keterlambatan, digital marketing untuk memprediksi pelanggan yang loyal atau pelanggan yang memiliki potensi untuk kembali membeli produk yang dijual, kesehatan untuk memprediksi apakah seorang pasien positif terkena penyakit tertentu, dan masih banyak lagi. Dari berbagai macam permasalahan klasifikasi tersebut tidak semua masalah klasifikasi memiliki jumlah target variabel yang seimbang (level yang mendominasi keseluruhan target merupakan kelas mayoritas dan level yang lebih kecil disebut kelas minoritas). Ketika kondisi seperti apa target variabel tidak memiliki proporsi yang seimbang ? Hal tersebut akan berpengaruh terhadap kemampuan model untuk memprediksi target (model klasifikasi cenderung lebih pintar dalam memprediksi kelas mayoritas), karena model klasifikasi sangat bergantung pada jumlah setiap level target dalam proses learning nya (model klasifikasi akan melalui proses learning yang seimbang jika diberikan jumlah sampel yang seimbang pula). Hal ini menjadi masalah yang cukup serius, sehingga perlu dilakukan penanganan sebagai solusi permasalahan tersebut. attrition &lt;- read_csv(&quot;data/02-C1/attrition.csv&quot;) %&gt;% mutate(attrition = as.factor(attrition)) prop.table(table(attrition$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.84 0.16 Salah satu cara yang paling umum dilakukan adalah menyeimbangkan jumlah target variabel dengan metode sampling. Metode tersebut terbagi menjadi 2, yaitu downsampling dan upsampling. Downsampling adalah proses sampling pada observasi kelas mayoritas sebanyak jumlah observasi pada kelas minoritas, tujuannya adalah menyamakan jumlah observasi pada kelas mayoritas dan minoritas. Sehingga model klasifikasi dapat melalui proses learning yang seimbang. Proses downsampling akan mengurangi jumlah observasi pada kelas mayoritas, sehingga memungkinkan terjadinya kehilangan informasi. Upsampling adalah proses sampling pada observasi kelas minoritas sebanyak jumlah observasi pada kelas mayoritas, tujuannya adalah menyamakan jumlah observasi pada kelas mayoritas dan minoritas. Sehingga model klasifikasi dapat melalui proses learning yang seimbang. Proses upsampling akan menambah jumlah observasi pada kelas minoritas, sehingga memungkinkan terdapat data yang duplicate pada kelas minoritas. Untuk melakukan downsampling dan upsampling dapat menggunakan fungsi pada library caret ataupun recipes. Berikut contoh downsampling dan upsampling dengan menggunakan fungsi pada library caret dan recipes Sebelum menerapkan downsampling dan upsamling terlebih dahulu dilakukan cross validation, yaitu membagi data menjadi training set untuk proses pemodelan dan testing set untuk melakukan evaluasi. Cross validation akan dilakukan dengan menggunakan fungsi initial_split() dari library rsample. Fungsi tersebut akan melakukan proses sampling untuk cross validation dengan metode stratified random sampling, sehingga proporsi target variabel pada data awal, akan dipertahankan baik pada training set maupun testing set. # define seed set.seed(100) # menentukan indeks untuk train dan test splitted &lt;- initial_split(data = attrition, prop = 0.75, strata = &quot;attrition&quot;) # mengambil indeks data train dengan fungsi `tarining()` train &lt;- training(splitted) # mengambil indeks data test dengan fungsi `testing()` test &lt;- testing(splitted) prop.table(table(train$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.84 0.16 prop.table(table(test$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.84 0.16 Downsampling dan upsampling hanya akan dilakukan pada data train karena proses pembuatan model klasifikasi hanya dilakukan pada data train. Data test hanya digunakan untuk mengevaluasi model yang dihasilkan pada data train. 2.1.1 Downsample Untuk melakukan downsampling dengan library caret dapat menggunakan fungsi downSample(). train_down &lt;- downSample(x = train[, -1], y = train$attrition, yname = &quot;attrition&quot;) prop.table(table(train_down$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.5 0.5 2.1.2 Upsample Untuk melakukan upsampling dengan library caret dapat menggunakan fungsi upSample(). train_up &lt;- upSample(x = train[, -1], y = train$attrition, yname = &quot;attrition&quot;) prop.table(table(train_up$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.5 0.5 Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi downSample: Down- and Up-Sampling Imbalanced Data 2.1.3 Downsample/Upsample Using Recipes Sama seperti saat menggunakan fungsi pada library caret, ketika menggunakan fungsi dari library recipes juga harus dilakukan cross validation terlebih dahulu. Perbedaan ketika menggunakan fungsi dari library recipes data train dan data test tidak langsung dimasukkan ke dalam sebuah objek melainkan dilakukan downsampling atau upsampling terlebih dahulu. set.seed(417) splitted_rec &lt;- initial_split(data = attrition, prop = 0.8, strata = &quot;attrition&quot;) splitted_rec #&gt; &lt;1177/293/1470&gt; Untuk melakukan downsampling atau upsampling menggunakan library recipes dapat menggunakan fungsi step_downsample() atau step_upsample() yang didefinisikan dalam sebuah recipe. rec &lt;- recipe(attrition ~ ., training(splitted)) %&gt;% # `step_downsample()` dapat diganti dengan `step_upsample()` step_downsample(attrition, ratio = 1, seed = 100) %&gt;% prep() # membuat data train dengan fungsi `juice()` train_rec &lt;- juice(rec) # membuat data test dengan fungsi `bake()` test_rec &lt;- bake(rec, testing(splitted)) prop.table(table(train_rec$attrition)) %&gt;% round(2) #&gt; #&gt; no yes #&gt; 0.5 0.5 Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi tidymodels/recipes Apa yang dimaksud dengan false positive dan false negative? False positive adalah kasus dimana sisi negatif terprediksi sebagai positif. Contohnya, pasien terprediksi mengidap kanker ganas, namun data actual nya pasien tersebut mengidap kanker jinak. False negative adalah kasus dimana sisi positif terprediksi sebagai negatif. Contohnya, pasien teprediksi mengidap kanker jinak, namun data actual nya pasien tersebut mengidap kanker ganas. Bagaimana model regresi logistic regression menangani data kategorik? Sama seperti kasus linear regression, pada logistic regression akan mengubah variabel kategorik menjadi dummy variabel. Apa maksud dari nilai AIC? AIC (Akaike Information Criterion) menggambarkan seberapa banyak informasi yang hilang pada model tersebut. AIC biasa digunakan untuk membandingkan beberapa model, karena berbeda dengan R-squared yang memiliki range semakin mendekati 1 semakin baik, AIC tidak memiliki batasan, jadi kita perlu membandingkannya dengan model lain. Jelaskan kegunaan dari ROC dan AUC? Kurva ROC (Receiver Operating Characteristic) menggambarkan seberapa baik kinerja model klasifikasi biner. Kurva ROC dibentuk dari nilai TPR (True Positive Rate) dan FPR (False Positive Rate) untuk semua nilai threshold dari 0 hingga 1. AUC (Area Under the Curve) adalah luas daerah dari kurva ROC. Nilai AUC mendekati 1 artinya model sangat baik, ketika nilai AUC berada di sekitar 0.5 maka model tersebut memiliki performance yang tidak baik dan cenderung hanya menebak secara acak. Chapter 3 Classification 2 # libraries library(tidyverse) library(e1071) library(tm) library(katadasaR) library(textclean) Dari berbagai metode klasifikasi yang telah dipelajari (logistic regression, KNN, naive bayes, decision tree, dan random forest), bagaimana pemilihan dalam penggunaan metode tersebut? Pemilihan metode klasifikasi bergantung pada tujuan analisis yang dilakukan. Secara umum tujuan pemodelan klasifikasi adalah melakukan analisa terkait hubungan prediktor dengan target variabel atau melakukan prediksi. Jika tujuannya adalah untuk melakukan analisa terkait hubungan antara prediktor dan target variabel dapat menggunakan logistic regression atau decision tree. Berikut kelebihan dan kekurangan dari kedua metode tersebut. Logistic regression: model klasifikasi yang cukup sederhana dan komputasinya cepat (+) interpretable (+) tidak mengharuskan scaling data (+) baseline yang baik sebelum mencoba model yang lebih kompleks (+) memerlukan kejelian saat melakukan feature engineering karena sangat bergantung pada data yang fit tidak dapat mengevaluasi hubungan yang tidak linier (-) mengharuskan antar prediktornya tidak saling terkait (cukup kaku) (-) Decision tree: tidak mengharuskan scaling data (+) dapat mengevaluasi hubungan yang tidak linier (+) antar prediktornya boleh saling berkaitan (+) interpretable (+) decision tree yang terbentu cenderung tidak stabil (sedikit perubahan pada data akan merubah struktur pohon yang dihasilkan) (-) komputasi relatif lebih lama (-) cenderung overfitting (-) Jika tujuannya adalah melakukan prediksi dengan harapan tingkat akurasi yang tinggi, bisa menggunakan random forest. Karena metode ini merupakan metode klasifikasi yang menggabungkan beberapa metode, sehingga cukup robust (tidak sensitif) terhadap outlier, antar prediktor boleh saling berkaitan, bahkan mengatasi overfitting. Naive bayes umumnya digunakan untuk masalah-masalah yang berkaitan dengan klasifikasi text. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi The Naive Bayes Classifier Apakah k-fold cross validation dapat digunakan untuk metode klasifikasi lain (selain random forest)? k-fold cross validation dapat digunakan untuk semua metode klasifikasi bahkan di luar metode klasifikasi yang telah dipelajari. Namun, karena k-fold cross validation tidak memperlihatkan hasil pemodelan untuk semua subset data (hanya mengambil yang terbaik, yaitu tingkat akurasi tertinggi), maka tetap perlu dilakukan cross validation untuk melakukan evaluasi model. Berikut contoh k-fold cross validation untuk metode lain (selain random forest). set.seed(417) ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3) # parameter method dapat disesuaikan dengan metode klasifikasi yang digunakan model &lt;- train(attrition ~ ., data = train, method = &quot;ctree&quot;, trControl = ctrl) Apakah pada metode KNN, naive bayes, decision tree, dan random forest hasilnya dapat berupa probability? Pada dasarnya semua metode klasifikasi akan menghasilkan probability, bukan langsung kelas. Namun, kebanyakan metode klasifikasi secara default di R langsung menghasilkan kelas (threshold 0.5). Untuk menghasilkan probability dapat menambahkan parameter type saat mealukan predict(). Berikut beberapa type untuk metode klasifikasi yang dipelajari: response untuk logistic regression raw untuk naive bayes probability untuk decision tree dan random forest Apakah metode naive bayes dapat diterapkan untuk prediktor bertipe numerik? peluang apa yang dihitung? Naive bayes dapat diterapkan pada berbagai permasalahan klasifikasi bukan hanya pada klasifikasi text. Jika prediktor yang digunakan bertipe numerik naive bayes akan menghitung peluang rata-rata (mean) dan standard deviation (sd) untuk setiap level target. Berikut contoh naive bayes pada data iris. naiveBayes(Species ~ ., iris) #&gt; #&gt; Naive Bayes Classifier for Discrete Predictors #&gt; #&gt; Call: #&gt; naiveBayes.default(x = X, y = Y, laplace = laplace) #&gt; #&gt; A-priori probabilities: #&gt; Y #&gt; setosa versicolor virginica #&gt; 0.3333333 0.3333333 0.3333333 #&gt; #&gt; Conditional probabilities: #&gt; Sepal.Length #&gt; Y [,1] [,2] #&gt; setosa 5.006 0.3524897 #&gt; versicolor 5.936 0.5161711 #&gt; virginica 6.588 0.6358796 #&gt; #&gt; Sepal.Width #&gt; Y [,1] [,2] #&gt; setosa 3.428 0.3790644 #&gt; versicolor 2.770 0.3137983 #&gt; virginica 2.974 0.3224966 #&gt; #&gt; Petal.Length #&gt; Y [,1] [,2] #&gt; setosa 1.462 0.1736640 #&gt; versicolor 4.260 0.4699110 #&gt; virginica 5.552 0.5518947 #&gt; #&gt; Petal.Width #&gt; Y [,1] [,2] #&gt; setosa 0.246 0.1053856 #&gt; versicolor 1.326 0.1977527 #&gt; virginica 2.026 0.2746501 Bagaimana cara menghapus stopwords dalam bahasa indonesia? Download terlebh dahulu file.txt (stopwords id.txt) yang berisi stopwords bahasa indonesia di internet. Kemudian import stopwords id.txt tersebut dengan menggunakan fungsi readLines(). # import Indonesia stopwords stop_id &lt;- readLines(&quot;data/03-C2/stopwords_id.txt&quot;) # generate data frame text &lt;- data.frame(sentence = c(&quot;saya tertarik belajar data science di @algoritma :)&quot;, &quot;anda tinggal di Jakarta&quot;, &quot;Ingin ku meratüî• naüëç&quot;, &quot;selamat tahun baru #2020 !&quot;, &quot;pingin makan yang kek gitu&quot;)) Mengubah text dalam bentuk data frame ke bentuk corpus dengan menggunakan fungsi VectorSource() dan VCorpus() dari library tm. Setelah itu, baru dapat menghapus stopwords dengan menggabungkan fungsi tm_map() dan removeWords(). text_clean1 &lt;- text %&gt;% pull(sentence) %&gt;% VectorSource() %&gt;% VCorpus() %&gt;% tm_map(removeWords, stop_id) text_clean1[[1]]$content #&gt; [1] &quot; tertarik belajar data science @algoritma :)&quot; Bagaimana cara mengubah kata berimbuhan mejadi kata dasarnya saja dalam bahasa Indonesia? Untuk mengubah kata berimbuhan menjadi kata dasar dalam bahasa Indonesia dapat menggunakan fungsi katadasaR() dari library katadasaR. Namun, fungsi tersebut hanya menerima 1 inputan (1 value) saja sehigga dibutuhkan fungsi sapply() untuk mengaplikasikan fungsi tersebut ke dalam 1 kalimat. # membuat fungsi untuk mengubah kata berimbuhan menjadi kata dasar kata_dasar &lt;- function(x) { paste(sapply(words(x), katadasaR), collapse = &quot; &quot;) } Menggunakan fungsi di atas dengan menggabungkan fungsi tm_map() dan content_transformer(). text_clean2 &lt;- text %&gt;% pull(sentence) %&gt;% VectorSource() %&gt;% VCorpus() %&gt;% tm_map(content_transformer(kata_dasar)) text_clean2[[1]]$content #&gt; [1] &quot;saya tarik ajar data science di @algoritma :)&quot; Bagaiamana cara menghapus emoticon dan emoji? Untuk menghapus emoticon dan emoji dapat menggunakan fungsi replace_emoji() dan replace_emoticon() dari library textclean. Namun, fungsi tersebut hanya menerima tipe data berupa karakter sehingga harus diubah terlebih dahulu tipe datanya jika masih belum karakter. text_clean3 &lt;- text %&gt;% mutate(sentence = as.character(sentence)) %&gt;% pull(sentence) %&gt;% replace_emoji() %&gt;% replace_emoticon() text_clean3 #&gt; [1] &quot;saya tertarik belajar data science di @algoritma smiley &quot; #&gt; [2] &quot;anda tinggal di Jakarta&quot; #&gt; [3] &quot;Ingin ku merat&lt;U+0001F525&gt; na&lt;U+0001F44D&gt;&quot; #&gt; [4] &quot;selamat tahun baru #2020 !&quot; #&gt; [5] &quot;pingin makan yang kek gitu&quot; Bagaimana cara menghapus mention dan hashtag? Untuk menghapus mention dan hashtag dapat menggunakan fungsi replace_hash() dan replace_tag() dari library textclean. text_clean4 &lt;- text %&gt;% mutate(sentence = as.character(sentence)) %&gt;% pull(sentence) %&gt;% replace_hash() %&gt;% replace_tag() text_clean4 #&gt; [1] &quot;saya tertarik belajar data science di :)&quot; #&gt; [2] &quot;anda tinggal di Jakarta&quot; #&gt; [3] &quot;Ingin ku merat&lt;U+0001F525&gt; na&lt;U+0001F44D&gt;&quot; #&gt; [4] &quot;selamat tahun baru !&quot; #&gt; [5] &quot;pingin makan yang kek gitu&quot; Bagaimana cara menghapus slang words? Untuk menghapus slang words dapat menggunakan fungsi replace_internet_slang() dari library textclean. slang_id &lt;- read.csv(&quot;data/03-C2/colloquial-indonesian-lexicon.csv&quot;) text_clean5 &lt;- text %&gt;% mutate(sentence = as.character(sentence)) %&gt;% pull(sentence) %&gt;% replace_internet_slang(slang = paste0(&#39;\\\\b&#39;, slang_id$slang, &#39;\\\\b&#39;) , replacement = slang_id$formal, ignore.case = T) text_clean5 #&gt; [1] &quot;saya tertarik belajar data science di @algoritma :)&quot; #&gt; [2] &quot;anda tinggal di Jakarta&quot; #&gt; [3] &quot;Ingin ku merat&lt;untuk+0001F525&gt; nya&lt;untuk+0001F44D&gt;&quot; #&gt; [4] &quot;selamat tahun baru #2020 !&quot; #&gt; [5] &quot;pengin makan yang kayak begitu&quot; Berikut link eksternal yang dapat dijadikan sebagai bahan referensi dalam melakukan cleaning text TEXT CLEANING BAHASA INDONESIA dan TEXT CLEANING BAHASA INGGRIS Chapter 4 Unsupervised Learning # libraries library(tidyverse) library(FactoMineR) library(plotly) Penerapan PCA di industri? PCA pada industri lebih sering digunakan untuk data preparation sama halnya seperti scaling, feature engineering, ataupun variable selection. PCA digunakan untuk mereduksi data besar menjadi data yang lebih kecil, secara sederhana dapat dikatakan mengurangi jumlah kolom pada data. Walaupun PCA mengurangi jumlah kolom pada data (mereduksi dimensi), PCA tetap mempertahankan semua variabel (menggunakan semua variabel). Sebelum mereduksi dimensi PCA akan merangkum terlebih dahulu semua informasi yang terdapat pada setiap variabel ke dalam bentuk PC, PC tersebut yang nantinya akan direduksi (dikurangi) dimensinya. Oleh karena itu, variabel yang digunakan jumlahnya tetap sama seperti data awal, hanya informasi (variansinya) saja yang berkurang. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi An Application of PCA. Contoh permasalahan yang sering ditemui adalah klasifikasi dokumen. Saat ini semua administrasi dilakukan secara online/elektronik (tidak manual), adakalanya seorang nasabah/pelamar/customer harus melakukan upload dokumen. Sebelum adanya klasifikasi dokumen, pengecekkan kebenaran dokumen dilakukan secara manual. sehingga membutuhkan waktu yang cukup lama dan kapasitas penyimpanan yang relatif besar karena apps tidak mampu memilah mana dokumen yang sudah sesuai dan mana yang belum. Namun, permasalahan tersebut sudah mampu terjawab dengan adanya klasifikasi dokumen. Data untuk klasifikasi dokumen adalah data image yang jika dilakukan proses klasifikasi akan memerlukan komputasi yang relatif lama dibandingkan data tabular biasa. Oleh karena itu, perlu dilakukan PCA untuk mereduksi dimensi data image tersebut supaya komputasi saat proses klasifikasi bisa menjadi lebih cepat. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi Image Compression with PCA in R. Apakah biplot dapat menampilkan PC lain selain PC1 dan PC2? Bisa tetapi informasi yang dijelaskan menjadi berkurang, karena secara default pada R PC1 dan PC2 merangkum informasi paling banyak. Berikut contoh membuat biplot dengan menggunakan PC lain (selain PC1 dan PC2): head(USArrests) #&gt; Murder Assault UrbanPop Rape #&gt; Alabama 13.2 236 58 21.2 #&gt; Alaska 10.0 263 48 44.5 #&gt; Arizona 8.1 294 80 31.0 #&gt; Arkansas 8.8 190 50 19.5 #&gt; California 9.0 276 91 40.6 #&gt; Colorado 7.9 204 78 38.7 Membuat PCA dari data USArrests dengan menggunakan fungsi prcomp(). pca_us &lt;- prcomp(USArrests, scale = T) Membuat visualisasi dari hasil PCA dengan menggunakan fungsi biplot(). # parameter `choices` dapat diganti sesuai PC yang ingin dibuat, secara default menggunakan PC1 dan PC2 (choices = 1:2) biplot(pca_us, choices = 2:3) Dimensionality reduction mengatasi masalah high-dimensional data, permasalahan apa yang terdapat pada data berdimensi tinggi? menyulitkan pengolahan data, memerlukan komputasi yang besar, tidak efisien secara waktu. Perbedaan membuat PCA dengan menggunakan fungsi prcomp() dan PCA() dari library FactoMiner? Fungsi untuk membuat biplot di R: biplot(prcomp()) -&gt; base R plot.PCA(PCA()) -&gt; package FactoMineR kelebihan ketika membuat PCA dengan menggunakan fungsi PCA() dari library FactoMiner adalah bisa membuat biplot lebih spesifik (memisah dua grafik yang diajdikan satu -&gt; individu/variabel) dan bisa mengkombinasikan antara variabel numerik dan kategorik dengan menggunakan fungsi plot.PCA(). Apakah terdapat best practice dalam menentukan jumlah PC yang digunakan pada PCA? Penentuan jumlah PC yang digunakan bergantung pada kebutuhan analisa yang dilakukan. Namun, kembali pada tujuan awal melakukan PCA, yaitu untuk mereduksi dimensi supaya analisis lanjutan yang dilakukan memiliki waktu yang relatif cepat dan ruang penyimpanan yang lebih efisien. Sehingga, seringkali seorang analis menentapkan threshold lebih dari 70-75% informasi. Maksudnya jumlah PC yang digunakan adalah jumlah PC yang sudah merangkum kurang lebih 70-75% informasi. Namun, threshold tersebut sifatnya tidak mutlak artinya disesuaikan dengan kebutuhan analisis dan bisnis. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi How many components can I retrieve in principal component analysis?. Bagaimana best practice dalam penentuan jumlah cluster? Penentuan jumlah cluster yang akan dibuat bergantung pada kebutuhan bisnis dalam industri. Namun, secara statistik penentuan jumlah cluster dapat dilakukan berdasarkan penurunan wss seperti yang telah dijelaskan pada pertanyaan sebelumnya. Hal yang perlu diperhatikan, yaitu jumlah cluster yang dipilih adalah jumlah cluster yang ketika dilakukan penambahan cluster sudah tidak mengakibatkan penurunan wss yang signifikan (pada grafik berbentuk landai), kemudian disesuaikan dengan kebutuhan bisnis pada industri. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi 10 Tips for Choosing the Optimal Number of Clusters Apakah kita dapat memvisualisasikan biplot dengan 3 dimensi? Untuk menampilkan biplot dengan 3 dimensi dapat menggunakan function plot_ly() dari package plotly. Berikut ini akan dicontohkan memvisualisasikan biplot dari PC1, PC2, PC3 dan juga akan dibedakan setiap titik observasi dengan cluster nya. Sebelum masuk ke visualisasi, akan dicari terlebih dahulu cluster untuk setiap observasi. # Read data in whiskies &lt;- read.csv(&quot;data/04-UL/whiskies.txt&quot;) # Distillery column is the name of each whisky rownames(whiskies) &lt;- whiskies[,&quot;Distillery&quot;] # remove RowID, Postcode, Latitude and Longitude whiskies &lt;- whiskies[,3:14] whi_km &lt;- kmeans(scale(whiskies), 4) Setelah menggunakan kmeans() untuk mendapatkan cluster, berikutnya kita lakukan PCA dan membentuk PC yang diperoleh dalam bentuk data frame. whis.pca&lt;-PCA(whiskies, graph = F,scale.unit = T) df_pca &lt;- data.frame(whis.pca$ind$coord) %&gt;% bind_cols(cluster = as.factor(whi_km$cluster)) head(df_pca) #&gt; Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 cluster #&gt; 1 -0.65565655 1.2056463 -0.1663438 -0.7807432 0.14526590 3 #&gt; 2 -2.31263102 3.7479878 1.3669186 0.8719922 0.69366566 3 #&gt; 3 -1.60215288 -0.6640822 -0.2972053 -1.1027897 -0.01535638 2 #&gt; 4 5.41363278 0.2448746 1.2101422 -0.7483052 -0.19536723 4 #&gt; 5 0.12164922 0.4127927 -0.3044621 -1.2705758 1.49597271 1 #&gt; 6 0.09941062 -1.3966133 -1.2024542 1.6549138 -0.28659985 1 Langkah berikutnya adalah memvisualisasikan PC dan membedakan warna observasi berdasarkan cluster nya. plot_ly(df_pca,x = ~Dim.1, y = ~Dim.2, z = ~Dim.3, color = ~cluster) Chapter 5 Time Series and Forecasting Apa itu Time Series? Time series merupakan data yang diperoleh dan disusun berdasarkan urutan waktu. Waktu yang digunakan dapat berupa hari, minggu, bulan, dan sebagainya. Apa itu Seasonal Effects? Seasonal effects terjadi jika data observasi memiliki pola yang berulang sesuai dengan siklus tertentu Perbedaan times series dan regression? Time series adalah analis variabel numerik berdasarkan deret waktu. Perbedaan mendasar dengan regresi, jika regresi memprediksi berdasarkan variabel independen lainnya (x1, x2, x3). Sedangkan untuk time series kita hanya mengamati variabel y yang akan kita prediksi Apa arti dari nilai smoothing parameter yang mendekati nilai 1? Nilai smoothing paremeter mendekati nilai 1 artinya bobot lebih besar diberikan ke data observasi terbaru Pada time series forecasting, data deret waktu yang dimiliki harus lengkap tanpa ada tanggal yang hilang, bagaimana mengatasi data yang tanggal nya tidak lengkap? library(lubridate) library(dplyr) Quantity &lt;- c(3,4,5) Order.Date &lt;- c(&quot;2019-01-03&quot;,&quot;2019-01-07&quot;,&quot;2019-01-08&quot;) dat &lt;- data.frame(Order.Date, Quantity) %&gt;% mutate(Order.Date = ymd(Order.Date)) Gunakan function pad() dari package padr untuk memenuhi tanggal yang hilang library(padr) dat %&gt;% pad() #&gt; Order.Date Quantity #&gt; 1 2019-01-03 3 #&gt; 2 2019-01-04 NA #&gt; 3 2019-01-05 NA #&gt; 4 2019-01-06 NA #&gt; 5 2019-01-07 4 #&gt; 6 2019-01-08 5 Bagaimana mengisi nilai NA pada time series object? Fill = \"extend\" adalah salah satu function untuk mengisi nilai NA dengan nilai disekitarnya. link library(zoo) dat %&gt;% pad() %&gt;% pull(Quantity) %&gt;% ts(frequency = 1) %&gt;% na.fill(fill = &quot;extend&quot;) #&gt; Time Series: #&gt; Start = 1 #&gt; End = 6 #&gt; Frequency = 1 #&gt; [1] 3.00 3.25 3.50 3.75 4.00 5.00 Jika hasil decomposition pada trend masih membentuk pola berulang, apa yang terjadi? Ketika hasil decomposition yang diperoleh pada trend masih membentuk pola berulang, itu artinya masih terdapat pola seasonal yang belum tertangkap, kemungkinan data yang digunakan memiliki multiple seasonal, untuk membuat object ts pada pola data multiple seasonal dapat menggunakan function msts(). Untuk penanganan multiple seasonal lebih lengkap nya dapat di cek pada link berikut ini. multiple seasonal Apakah pada metode arima kita dapat menambahkan variable prediktor pada analis? Untuk analisis time series dengan variabel prediktor lainnya dapat menggunakan parameter xreg pada function Arima() dan auto.arima(). library(fpp2) library(forecast) Arima(y = uschange[,1], xreg = uschange[,2], order = c(1,1,0)) #&gt; Series: uschange[, 1] #&gt; Regression with ARIMA(1,1,0) errors #&gt; #&gt; Coefficients: #&gt; ar1 xreg #&gt; -0.5412 0.1835 #&gt; s.e. 0.0638 0.0429 #&gt; #&gt; sigma^2 estimated as 0.3982: log likelihood=-177.46 #&gt; AIC=360.93 AICc=361.06 BIC=370.61 auto.arima(y = uschange[,1], xreg = uschange[,2]) #&gt; Series: uschange[, 1] #&gt; Regression with ARIMA(1,0,2) errors #&gt; #&gt; Coefficients: #&gt; ar1 ma1 ma2 intercept xreg #&gt; 0.6922 -0.5758 0.1984 0.5990 0.2028 #&gt; s.e. 0.1159 0.1301 0.0756 0.0884 0.0461 #&gt; #&gt; sigma^2 estimated as 0.3219: log likelihood=-156.95 #&gt; AIC=325.91 AICc=326.37 BIC=345.29 Untuk detail lengkapnya dapat di lihat pada link berikut ini: link Nilai error yang harus dilihat dan diperhatikan ? Kapan menggunakan MAE/RMSE/MAPE ? Tidak ada jawaban pasti untuk mengetahui penggunaan ketiga error tersebut. Tentunya setiap indikator memiliki memiliki kelebihan dan kekurangan masing-masing. Berikut ringkasan dari ketiga error tersebut: MAE(Mean Absolute Error), hasil MAE tidak akan terpengaruh jika memiliki data outlier. RMSE (Root Mean Square Error), memberikan jaminan untuk mendapatkan hasil perkiraan yang tidak bias karena cara hitungnya dengan mengkuadratkan error yang diperoleh, namun ketika memiliki data outlier pada data tentunya RMSE memiliki kecenderungan untuk memperoleh perkiraan yang besar. MAPE (Mean Absolute Percentage Error), MAPE menunjukan rata-rata kesalahan absolut peramalan dalam bentuk presentase terhadap data aktual. MAPE tidak cocok jika memiliki observasi yang bernilai 0, karena cara hitung MAPE adalah dengan membagi dengan nilai aktual, hal tersebut akan menyebabkan nilai MAPE menjadi infinit. Apakah hasil diff manual berbeda dengan yang dilakukan pada fungsi arima atau auto.arima ? Hasil prediksi yang diperoleh ketika melakukan differencing manual kemudian diaplikasikan dengan function arima/auto.arima akan sedikit berbeda ketika langsung melakukan differencing dari function arima/auto.arima. Hal ini tentunya tidak menjadi masalah besar, karena hasil yang diperoleh tidak jauh berbeda. Untuk detail rumus yang digunakan dapat dilihat di link berikut. differencing Chapter 6 Neural Network and Deep Learning Berapa jumlah hidden layer dan nodes untuk setiap hidden layer secara best practice dalam membangun arsitektur neural network (ANN) ? Kebanyakan orang menggunakan minimal 2 hidden layer, namun tidak menutup kemungkinan menggunakan lebih dari 2 ataupun kurang dari 2 hidden layer. Jumlah nodes biasanya semakin mengecil ketika hidden layers semakin dekat dengan output layer. Tujuannya adalah untuk melihat fitur dengan lebih spesifik. Kebanyakan orang menggunakan angka biner \\(2^{n}\\) seperti 1, 2, 4, 8, 16, 32, 64, 128, 256, dst karena neural network merupakan metode yang berasal dari orang computer science dan mathematics yang biasa menggunakan angka biner. Fungsi aktivasi apa yang sering digunakan ketika membuat arsitktur neural network ? Pada hidden layer biasa digunakan fungsi aktivasi relu karena relu melakukan transformasi data dengan mengubah nilai negatif menjadi 0 dan membiarkan nilai positif, sehingga semakin ke belakang informasi yang dibawa tidak banyak berkurang. Pada output layer: jika casenya adalah regresi digunakan fungsi aktivasi linear, jika casenya adalah klasifikasi biner digunakan fungsi aktivasi sigmoid, dan jika casenya adalah klasifikasi multiclass digunakan fungsi aktivasi softmax. Bagaimana menentukan batch size dan jumlah epoch ? Batch size biasanya menggunakan angka yang dapat membagi habis jumlah data, supaya data yang tersedia dapat digunakan secara keseluruhan (tidak ada yang tidak terpakai). Contoh: Jika data train terdiri dari 800 observasi, kita bisa menggunakan batch size 200 yang dapat embagai habis 80 observasi. Jumlah epoch dimulai dari angka yang kecil terlebih dahulu supaya komputasi yang dilakukan tidak terlalu lama, kemudian dilihat apakah error dan accuracy yang dhasilkan sudah konvergen atau belum. Jika belum bisa menambaha jumlah epoch sedikit demi sedikit, dan sebaliknya. Bagaimana menentukan learning rate yang tepat ? Learning rate berfungsi mempercepat atau memperlambat besaran update error. Semakin besar learning rate, maka error/accuracy akan semakin cepat konvergen. Namun, bisa saja titik error paling minimum (global optimum) terlewat. Semakin kecil learning rate, maka terdapat kemungkinan yang lebih besar untuk sampai di titik error paling minimum (global optimum). Namun, error/accuracy akan lebih lama konvergen. Optimizer apa yang paling sering digunakan ? Optimizer merupakan fungsi yang digunakan untuk mengoptimumkan error (memperkecil error). Secara sederhana untuk mengoptimumkan suatu fungsi bisa melalui fungsi turunan, pada neural network disebut sgd. Namun, sgd memiliki beberapa kekurangan sehingga mulai banyak yang memperbaiki fungsi sgd tersebut. Untuk sekarang ini salah satu optimizer yang cukup terkenal adalah adam sebagai optimizer yang merupakan perbaikan dari sgd karena optimizer tersebut dapat mengupdate/menyesuaikan momentum ketika proses optimisasi. Berikut link eksternal yang dapat dijadikan sebagai bahan referensi Adaptive Moment Estimation (Adam) Selain tips di atas berikut link eksternal yang dapat dijadikan referensi dalam membangun arsitektur neural network Rules-of-thumb for building a Neural Network Perbedaan metode-metode machine learning dengan neural network dan deep learning ? Neural network bukan merupakan metode yang berasal dari orang statistik melainkan lahir dari pemikiran orang-orang computer science dan math. Neural network merupakan salah satu metode machine learning, neural network yang arsitekturnya sudah cukup rumit sering disebut sebagai deep learning. Neural network memilki 1 hidden layer, sementara deep learning memiliki &gt; 1 hidden layer. Berikut merupakan link eksternal yang dapat dijadikan sebagai bahan referensi Deep learning &amp; Machine learning: what‚Äôs the difference? "]
]
